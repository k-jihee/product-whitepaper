from typing import Optional

import streamlit as st
import pandas as pd
import numpy as np
import re
import os
import io
import time
from typing import List, Tuple, Dict, Any

# Optional imports with graceful fallback
try:
    import git  # GitPython
    GIT_AVAILABLE = True
except Exception:
    GIT_AVAILABLE = False

try:
    import pdfplumber
    PDF_AVAILABLE = True
except Exception:
    PDF_AVAILABLE = False

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    SKLEARN_AVAILABLE = True
except Exception:
    SKLEARN_AVAILABLE = False

# =====================================
# ê¸°ë³¸ ì„¤ì •
# =====================================
st.set_page_config(page_title="Product Whitepaper AI", layout="wide")
APP_TITLE = "ğŸ¤– Product Whitepaper AI"
PROJECT_SUBTITLE = "GitHub ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì™€ ì±—ë´‡ì²˜ëŸ¼ ì§ˆì˜ì‘ë‹µ + ì œí’ˆë°±ì„œ ì „ìš© ë·°"
REPO_URL_DEFAULT = "https://github.com/k-jihee/product-whitepaper"

# ë¼ìš°íŒ… ê¸°ë³¸ê°’: ì²« í™”ë©´ì€ 'ì±—ë´‡'
if "route" not in st.session_state:
    st.session_state.route = "CHAT"  # CHAT, WHITEPAPER
if "authenticated" not in st.session_state:
    st.session_state.authenticated = True  # í•„ìš” ì‹œ ë¹„ë°€ë²ˆí˜¸ ì²˜ë¦¬ë¡œ ë³€ê²½ ê°€ëŠ¥
if "vector_ready" not in st.session_state:
    st.session_state.vector_ready = False

# =====================================
# ìœ í‹¸
# =====================================
def clean_int(value):
    try:
        cleaned = re.sub(r"[^\d.]", "", str(value))
        if cleaned == "":
            return "-"
        return f"{int(float(cleaned)):,} KG"
    except (ValueError, TypeError):
        return "-"

def parse_spec_text(spec_text):
    if pd.isna(spec_text):
        return {}
    lines = str(spec_text).splitlines()
    spec_dict = {}
    for line in lines:
        match = re.match(r"\s*\d+\.\s*(.+?)\s*:\s*(.+)", line)
        if match:
            key, value = match.groups()
            key = key.strip() if isinstance(key, str) else key
            spec_dict[key.strip()] = value.strip()
    return spec_dict

def format_features(text):
    if pd.isna(text):
        return "-"
    items = re.split(r"\s*-\s*", text.strip())
    items = [item for item in items if item]
    return "<br>".join(f"â€¢ {item.strip()}" for item in items)

def highlight_terms(text: str, query: str) -> str:
    if not query:
        return text
    try:
        pattern = re.compile("(" + re.escape(query) + ")", re.IGNORECASE)
        return pattern.sub(r"<mark>\1</mark>", text)
    except re.error:
        return text



# ----------------------------
# ê·œì¹™ ê¸°ë°˜ ì‘ë‹µ í•¨ìˆ˜ (RAG ì´ì „ ê³ ì • ë‹µ)
# ----------------------------
def get_rule_based_answer(query: str) -> Optional[str]:
    """íŠ¹ì • ì§ˆë¬¸ì— ëŒ€í•´ RAGë¥¼ ê±°ì¹˜ì§€ ì•Šê³  ê³ ì • ë‹µë³€ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not query:
        return None


# =====================================
# GitHub Repo ë™ê¸°í™”/ë¡œë”©
# =====================================
@st.cache_data(show_spinner=False)
def clone_or_update_repo(repo_url: str, repo_dir: str) -> Tuple[bool, str]:
    os.makedirs(repo_dir, exist_ok=True)
    if not GIT_AVAILABLE:
        return False, "GitPythonì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. 'pip install GitPython' í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."
    try:
        if os.path.isdir(os.path.join(repo_dir, ".git")):
            repo = git.Repo(repo_dir)
            origin = repo.remotes.origin
            origin.pull()
            return True, "ğŸ”„ ìµœì‹  ë‚´ìš©ìœ¼ë¡œ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤."
        else:
            git.Repo.clone_from(repo_url, repo_dir)
            return True, "âœ… ì €ì¥ì†Œë¥¼ í´ë¡ í–ˆìŠµë‹ˆë‹¤."
    except Exception as e:
        return False, f"âŒ ì €ì¥ì†Œ ë™ê¸°í™” ì˜¤ë¥˜: {e}"

def _read_text_file(path: str) -> str:
    try:
        with io.open(path, "r", encoding="utf-8") as f:
            return f.read()
    except UnicodeDecodeError:
        try:
            with io.open(path, "r", encoding="cp949") as f:
                return f.read()
        except Exception:
            return ""
    except Exception:
        return ""

def _read_pdf_text(path: str) -> str:
    if not PDF_AVAILABLE:
        return ""
    text = []
    try:
        with pdfplumber.open(path) as pdf:
            for page in pdf.pages:
                text.append(page.extract_text() or "")
    except Exception:
        return ""
    return "\n".join(text)

def _chunk_text(text: str, chunk_size: int = 900, overlap: int = 150) -> List[str]:
    if not text:
        return []
    tokens = re.split(r"(\s+)", text)
    chunks = []
    i = 0
    while i < len(tokens):
        window = tokens[i:i+chunk_size]
        chunks.append("".join(window).strip())
        i += max(1, chunk_size - overlap)
    return [c for c in chunks if c]

@st.cache_data(show_spinner=False)
def load_repo_corpus(repo_dir: str, exts: Tuple[str, ...] = (".md", ".txt", ".csv", ".json", ".pdf")) -> List[Dict[str, Any]]:
    corpus = []
    for root, _, files in os.walk(repo_dir):
        for fn in files:
            if not fn.lower().endswith(exts):
                continue
            path = os.path.join(root, fn)
            text = ""
            if fn.lower().endswith((".md", ".txt", ".csv", ".json")):
                text = _read_text_file(path)
            elif fn.lower().endswith(".pdf"):
                text = _read_pdf_text(path)
            if not text:
                continue
            # íŒŒì¼ ë‹¨ìœ„ chunking
            for idx, chunk in enumerate(_chunk_text(text)):
                corpus.append({
                    "path": path.replace(repo_dir, "").lstrip(os.sep),
                    "chunk_id": idx,
                    "text": chunk,
                })
    return corpus

@st.cache_resource(show_spinner=False)
def build_vector_store(corpus: List[Dict[str, Any]]):
    if not SKLEARN_AVAILABLE:
        return None, None
    texts = [c["text"] for c in corpus]
    vectorizer = TfidfVectorizer(
        ngram_range=(1, 2),
        max_features=100_000,
        min_df=1,
        stop_words=None
    )
    X = vectorizer.fit_transform(texts)
    return vectorizer, X

def retrieve(query: str, vectorizer, X, corpus: List[Dict[str, Any]], topk: int = 6) -> List[Dict[str, Any]]:
    if not (SKLEARN_AVAILABLE and vectorizer is not None and X is not None):
        # ë‹¨ìˆœ í‚¤ì›Œë“œ í•„í„°ë§ fallback
        q = query.lower()
        ranked = []
        for doc in corpus:
            score = (doc["text"].lower().count(q)) if q else 0
            if score > 0:
                ranked.append((score, doc))
        ranked.sort(key=lambda x: x[0], reverse=True)
        return [d for _, d in ranked[:topk]]
    q_vec = vectorizer.transform([query])
    sims = cosine_similarity(q_vec, X)[0]
    idxs = np.argsort(-sims)[:topk]
    results = []
    for i in idxs:
        doc = corpus[i].copy()
        doc["score"] = float(sims[i])
        results.append(doc)
    return results

def synthesize_answer(query: str, hits: List[Dict[str, Any]]) -> str:
    """
    ê°„ë‹¨ ìš”ì•½ê¸°: ê´€ë ¨ chunkë“¤ì„ ì—°ê²°í•´ ë‹µë³€ ì´ˆì•ˆì„ ë§Œë“­ë‹ˆë‹¤.
    (í•„ìš” ì‹œ OPENAI_API_KEY ì„¤ì •í•˜ì—¬ LLM í˜¸ì¶œ íŒŒì´í”„ë¼ì¸ì„ ì¶”ê°€í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)
    """
    if not hits:
        return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë‹¤ë¥´ê²Œ í•´ë³´ì‹œê² ì–´ìš”?"
    # ê°€ì¥ ìƒìœ„ 3ê°œ chunk ê¸°ë°˜ ì¶”ì¶œí˜• ì‘ë‹µ
    merged = "\n\n".join([h["text"] for h in hits[:3]])
    # ê¸¸ì´ ì œí•œ
    merged = merged[:2000]
    # ì¿¼ë¦¬ í•˜ì´ë¼ì´íŠ¸
    merged = highlight_terms(merged, query)
    prefix = "ì•„ë˜ëŠ” ì €ì¥ì†Œ ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ ê´€ë ¨ ë‚´ìš©ì…ë‹ˆë‹¤:\n\n"
    return prefix + merged

# =====================================
# ì œí’ˆë°±ì„œ ë¡œë”©
# =====================================
@st.cache_data(show_spinner=False)
def load_product_df():
    try:
        df = pd.read_csv("product_data.csv", encoding="utf-8")
        if "ìš©ë„" in df.columns:
            df["ìš©ë„"] = df["ìš©ë„"].astype(str).str.replace(r"\s*-\s*", " / ", regex=True)
        # ê³„ì¸µ ìë™ ìƒì„±
        if "ê³„ì¸µêµ¬ì¡°_2ë ˆë²¨" not in df.columns or "ê³„ì¸µêµ¬ì¡°_3ë ˆë²¨" not in df.columns:
            def get_hierarchy(code):
                if pd.isna(code):
                    return "ê¸°íƒ€", "ê¸°íƒ€"
                code = str(code)
                if code.startswith("GIB"):
                    return "FG0009 : ë¶€ì‚°ë¬¼", "ë¶€ì‚°ë¬¼"
                elif code.startswith(("GID1","GID2","GID3")):
                    return "FG0001 : í¬ë„ë‹¹", "í¬ë„ë‹¹ë¶„ë§"
                elif code.startswith(("GID6","GID7")):
                    return "FG0001 : í¬ë„ë‹¹", "í¬ë„ë‹¹ì•¡ìƒ"
                elif code.startswith("GIS62"):
                    return "FG0002 : ë¬¼ì—¿", "ê³ ê°ë¯¸75"
                elif code.startswith(("GIS601","GIS631")):
                    return "FG0002 : ë¬¼ì—¿", "ê³ ê°ë¯¸82"
                elif code.startswith(("GIS701","GIS703")):
                    return "FG0002 : ë¬¼ì—¿", "ì¼ë°˜75"
                elif code.startswith("GIS401"):
                    return "FG0002 : ë¬¼ì—¿", "ì¼ë°˜82"
                elif code.startswith("GIS201"):
                    return "FG0002 : ë¬¼ì—¿", "ì €ë‹¹ë¬¼ì—¿"
                elif code.startswith("GIS22"):
                    return "FG0002 : ë¬¼ì—¿", "ì œë„¤ë±ìŠ¤"
                elif code.startswith("GIS23"):
                    return "FG0002 : ë¬¼ì—¿", "ê°€ë£¨ì—¿"
                elif code.startswith("GIS90"):
                    return "FG0002 : ë¬¼ì—¿", "ë§¥ì•„82"
                elif code.startswith("GIS92"):
                    return "FG0002 : ë¬¼ì—¿", "ë§¥ì•„75"
                elif code.startswith("GIS93"):
                    return "FG0002 : ë¬¼ì—¿", "í•˜ì´ë§í† ìŠ¤"
                elif code.startswith(("GIF501","GIF502")):
                    return "FG0003 : ê³¼ë‹¹", "55%ê³¼ë‹¹"
                elif code.startswith("GIC002"):
                    return "FG0004 : ì „ë¶„", "ì¼ë°˜ì „ë¶„"
                elif str(code).startswith(("GIC","GIT")):
                    return "FG0004 : ì „ë¶„", "ë³€ì„±ì „ë¶„"
                elif code.startswith("GISQ190"):
                    return "FG0006 : ì•Œë£°ë¡œìŠ¤", "ì•Œë£°ë¡œìŠ¤ ì•¡ìƒ"
                elif code.startswith(("GIN121","GIN1221")):
                    return "FG0007 : ì˜¬ë¦¬ê³ ë‹¹", "ì´ì†Œë§í† ì˜¬ë¦¬ê³  ì•¡ìƒ"
                elif code.startswith(("GIN1230","GIN1220")):
                    return "FG0007 : ì˜¬ë¦¬ê³ ë‹¹", "ì´ì†Œë§í† ì˜¬ë¦¬ê³  ë¶„ë§"
                elif code.startswith("GIN131"):
                    return "FG0007 : ì˜¬ë¦¬ê³ ë‹¹", "ê°ˆë½í† "
                elif code.startswith("GIN151"):
                    return "FG0007 : ì˜¬ë¦¬ê³ ë‹¹", "ë§í† ì˜¬ë¦¬ê³ "
                elif code.startswith(("GIP202","GIP204")):
                    return "FG0008 : ì‹ì´ì„¬ìœ ", "í´ë¦¬ë±ìŠ¤íŠ¸ë¡œìŠ¤"
                elif code.startswith(("GIS242","GIS240")):
                    return "FG0008 : ì‹ì´ì„¬ìœ ", "NMD ì•¡ìƒ/ë¶„ë§"
                else:
                    return "ê¸°íƒ€", "ê¸°íƒ€"
            df[["ê³„ì¸µêµ¬ì¡°_2ë ˆë²¨", "ê³„ì¸µêµ¬ì¡°_3ë ˆë²¨"]] = df["ì œí’ˆì½”ë“œ"].apply(lambda x: pd.Series(get_hierarchy(x)))
        return df
    except Exception as e:
        st.error(f"âŒ product_data.csv ë¶ˆëŸ¬ì˜¤ê¸° ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

def product_card(row):
    prod_2022 = clean_int(row.get('ìƒì‚°ì‹¤ì (2022)'))
    prod_2023 = clean_int(row.get('ìƒì‚°ì‹¤ì (2023)'))
    prod_2024 = clean_int(row.get('ìƒì‚°ì‹¤ì (2024)'))
    internal_spec = parse_spec_text(row.get("ì‚¬ë‚´ê·œê²©(COA)", ""))
    legal_spec = parse_spec_text(row.get("ë²•ì ê·œê²©", ""))
    all_keys = set(internal_spec.keys()) | set(legal_spec.keys()) | {"ì„±ìƒ"}
    ì„±ìƒ_row = '<tr><td>ì„±ìƒ</td><td colspan="2">{}</td></tr>'.format(row.get("ì„±ìƒ", "-"))
    spec_rows = ""
    for key in sorted(all_keys):
        if key == "ì„±ìƒ":
            continue
        legal = legal_spec.get(key, "-")
        internal = internal_spec.get(key, "-")
        spec_rows += f"<tr><td>{key}</td><td>{legal}</td><td>{internal}</td></tr>"
    img_links = str(row.get("í•œë„ê²¬ë³¸", "")).strip()
    if img_links in ["", "í•œë„ê²¬ë³¸ ì—†ìŒ"]:
        sample_html = "í•´ë‹¹ì‚¬í•­ ì—†ìŒ"
    else:
        imgs = "".join(
            f'<img src="{link.strip()}" width="500" onclick="showModal(this.src)" style="cursor:pointer; margin:10px;">'
            for link in img_links.split(",") if link.strip()
        )
        sample_html = f"""
        <div style="text-align:left;">
            {imgs}
            <div style="margin-top: 10px;">
                <button onclick="printSample()">ğŸ–¨ï¸ í•œë„ê²¬ë³¸ë§Œ PDFë¡œ ì €ì¥</button>
            </div>
        </div>
        """
    html_template = f"""<style>
    table {{ table-layout: fixed; width: 100%; border-collapse: collapse; }}
    th, td {{ border: 1px solid gray; padding: 8px; text-align: center; }}
    th {{ background-color: #f2f2f2; }}
    @media print {{ button {{ display: none; }} }}
    #modal {{ display:none; position:fixed; left:0; top:0; width:100vw; height:100vh; background:rgba(0,0,0,0.7); align-items:center; justify-content:center; }}
    </style>
    <div id='print-area'>
    <h2>{row.get('ì œí’ˆëª…', '-')}</h2>
    <p><b>ìš©ë„:</b> {row.get('ìš©ë„', '-')}</p>
    <h3>1. ì œí’ˆ ì •ë³´</h3>
    <table>
    <tr><th>ì‹í’ˆìœ í˜•</th><th>ì œí’ˆêµ¬ë¶„</th><th>ì œí’ˆì½”ë“œ</th><th>ì†Œë¹„ê¸°í•œ</th></tr>
    <tr><td>{row.get('ì‹í’ˆìœ í˜•', '-')}</td><td>{row.get('êµ¬ë¶„', '-')}</td><td>{row.get('ì œí’ˆì½”ë“œ', '-')}</td><td>{row.get('ì†Œë¹„ê¸°í•œ', '-')}</td></tr>
    </table>
    <h3>ğŸ“Š ìƒì‚°ëŸ‰ (3ê°œë…„)</h3>
    <table><tr><th>2022</th><th>2023</th><th>2024</th></tr><tr><td>{prod_2022}</td><td>{prod_2023}</td><td>{prod_2024}</td></tr></table>
    <h3>2. ì£¼ìš”ê±°ë˜ì²˜</h3><p>{row.get('ì£¼ìš”ê±°ë˜ì²˜', '-')}</p>
    <h3>3. ì œì¡°ë°©ë²•</h3><p>{row.get('ì œì¡°ë°©ë²•', '-')}</p>
    <h3>4. ì›ì¬ë£Œëª… ë° í•¨ëŸ‰ / ì›ì‚°ì§€</h3><p>{row.get('ì›ì¬ë£Œëª… ë° í•¨ëŸ‰', '-')} / {row.get('ì›ì‚°ì§€', '-')}</p>
    <h3>5. ì œí’ˆ íŠ¹ì§•</h3><p>{format_features(row.get('ì œí’ˆíŠ¹ì§•', '-'))}</p>
    <h3>6. ì œí’ˆ ê·œê²©</h3>
    <table><tr><th>í•­ëª©</th><th>ë²•ì ê·œê²©</th><th>ì‚¬ë‚´ê·œê²©</th></tr>{ì„±ìƒ_row}{spec_rows}</table>
    <h3>7. ê¸°íƒ€ì‚¬í•­</h3><p>{row.get('ê¸°íƒ€ì‚¬í•­', '-')}</p></div>
    <div id='sample-area'><h3>8. í•œë„ê²¬ë³¸</h3>{sample_html}</div>
    <div id="modal" onclick="this.style.display='none'"><img id="modal-img" style="max-width:90%; max-height:90%; object-fit:contain;"></div>
    <script>
    function printSample() {{
        const original = document.body.innerHTML;
        const printSection = document.getElementById("sample-area").innerHTML;
        document.body.innerHTML = printSection;
        window.print();
        document.body.innerHTML = original;
    }}
    function showModal(src) {{
        document.getElementById("modal-img").src = src;
        document.getElementById("modal").style.display = "flex";
    }}
    </script>
    <br><button onclick="window.print()">ğŸ–¨ï¸ ì´ ì œí’ˆë°±ì„œ í”„ë¦°íŠ¸í•˜ê¸°</button>"""
    st.components.v1.html(html_template, height=2200, scrolling=True)

# =====================================
# í˜ì´ì§€: ì±—ë´‡
# =====================================
def page_chatbot():
    st.title("ğŸ’¬ ì±—ë´‡ ì „ìš©")
    st.caption("GitHub ì €ì¥ì†Œì˜ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆì˜ì‘ë‹µí•©ë‹ˆë‹¤. (NotebookLM ìœ ì‚¬ RAG)")

    with st.expander("âš™ï¸ ë°ì´í„° ì†ŒìŠ¤ ì„¤ì •", expanded=False):
        repo_url = st.text_input("GitHub Repo URL", value=os.environ.get("REPO_URL", REPO_URL_DEFAULT))
        repo_dir = st.text_input("ë¡œì»¬ ìºì‹œ í´ë”", value=os.environ.get("REPO_CACHE_DIR", "repo_cache"))
        colA, colB, colC = st.columns(3)
        with colA:
            sync = st.button("ğŸ”„ ì €ì¥ì†Œ ë™ê¸°í™” (clone/pull)")
        with colB:
            reset_index = st.button("ğŸ§¹ ì¸ë±ìŠ¤ ì¬ìƒì„±")
        with colC:
            show_stats = st.checkbox("ë¬¸ì„œ í†µê³„ ë³´ê¸°", value=False)

        status_box = st.empty()
        if sync:
            ok, msg = clone_or_update_repo(repo_url, repo_dir)
            status_box.info(msg)
            if ok:
                st.cache_data.clear()
                st.cache_resource.clear()
                st.session_state.vector_ready = False

        corpus = load_repo_corpus(repo_dir)
        if show_stats:
            st.write(f"ë¬¸ì„œ ì²­í¬ ìˆ˜: {len(corpus)}")
            sample_paths = sorted(list({c['path'] for c in corpus}))[:20]
            st.write("ì˜ˆì‹œ íŒŒì¼:", sample_paths)

        if (not st.session_state.vector_ready) and corpus:
            vectorizer, X = build_vector_store(corpus)
            st.session_state.vectorizer = vectorizer
            st.session_state.X = X
            st.session_state.corpus = corpus
            st.session_state.vector_ready = True
            status_box.success("âœ… ì¸ë±ìŠ¤ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")

        if reset_index:
            st.cache_resource.clear()
            if corpus:
                vectorizer, X = build_vector_store(corpus)
                st.session_state.vectorizer = vectorizer
                st.session_state.X = X
                st.session_state.corpus = corpus
                st.session_state.vector_ready = True
                status_box.success("ğŸ” ì¸ë±ìŠ¤ë¥¼ ì¬ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

    # ì±„íŒ… UI
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ì˜ˆ) 'ì •ì œí¬ë„ë‹¹ CCP ì•Œë ¤ì¤˜', 'GIS703 ìš©ë„'"}]

    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    query = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
    if query:
        st.session_state.messages.append({"role": "user", "content": query})
        with st.chat_message("user"):
            st.markdown(query)

        with st.chat_message("assistant"):
            placeholder = st.empty()
            placeholder.markdown("ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤...")
            # 1) ê·œì¹™ ê¸°ë°˜(ê³ ì • ë©˜íŠ¸) ìš°ì„  ì ìš©
            rule_answer = get_rule_based_answer(query)
            if rule_answer:
                placeholder.markdown(rule_answer)
                st.session_state.messages.append({"role": "assistant", "content": rule_answer})
            else:
                corpus = st.session_state.get("corpus", [])
                vectorizer = st.session_state.get("vectorizer", None)
                X = st.session_state.get("X", None)

                hits = retrieve(query, vectorizer, X, corpus, topk=6) if corpus else []
                answer = synthesize_answer(query, hits)
                # ê²°ê³¼ ë Œë”
                placeholder.markdown(answer, unsafe_allow_html=True)
                if hits:
                    with st.expander("ğŸ” ì°¸ì¡° ë¬¸ì„œ (ìƒìœ„ 6ê°œ)"):
                        for h in hits:
                            score_txt = f" | score={h.get('score', 0):.3f}" if "score" in h else ""
                            st.markdown(f"**{h['path']}** (chunk #{h['chunk_id']}){score_txt}")
                            st.code(h["text"][:800])

                st.session_state.messages.append({"role": "assistant", "content": answer})
    st.divider()
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ§» ëŒ€í™” ì´ˆê¸°í™”"):
            st.session_state.messages = [{"role": "assistant", "content": "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"}]
            st.experimental_rerun()
    with col2:
        st.caption("â€» ê³ ê¸‰ ìš”ì•½/ìƒì„±ì€ OPENAI_API_KEY ì—°ë™ í›„ í™•ì¥ ê°€ëŠ¥")

# =====================================
# í˜ì´ì§€: ì œí’ˆë°±ì„œ
# =====================================
def page_whitepaper():
    st.title("ğŸ“˜ ì œí’ˆë°±ì„œ ì „ìš©")
    df = load_product_df()

    with st.expander("ğŸ“‹ ì „ì œí’ˆ ëª©ë¡", expanded=False):
        if not df.empty:
            st.dataframe(df[["ê³„ì¸µêµ¬ì¡°_2ë ˆë²¨","ê³„ì¸µêµ¬ì¡°_3ë ˆë²¨","ì œí’ˆì½”ë“œ","ì œí’ˆëª…"]].dropna().reset_index(drop=True), use_container_width=True)
        else:
            st.info("product_data.csv ë¥¼ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ë‘ë©´ ëª©ë¡ì´ í‘œì‹œë©ë‹ˆë‹¤.")

    st.markdown("---")
    st.markdown('<h4>ğŸ” <b>ì œí’ˆì½”ë“œ ë˜ëŠ” ì œí’ˆëª…ì„ ì…ë ¥í•˜ì„¸ìš”</b></h4>', unsafe_allow_html=True)
    col1, col2 = st.columns(2)
    with col1:
        q1 = st.text_input("ğŸ” ì œí’ˆ 1 (ì˜ˆ: GIB1010 ë˜ëŠ” ê¸€ë£¨í…í”¼ë“œ)")
    with col2:
        q2 = st.text_input("ğŸ” ì œí’ˆ 2 (ì˜ˆ: GIS7030 ë˜ëŠ” ë¬¼ì—¿)")
    queries = [q for q in [q1, q2] if q]

    if queries and not df.empty:
        results = pd.DataFrame()
        for q in queries:
            partial = df[
                df["ì œí’ˆì½”ë“œ"].astype(str).str.contains(q, case=False, na=False) |
                df["ì œí’ˆëª…"].astype(str).str.contains(q, case=False, na=False)
            ]
            results = pd.concat([results, partial])
        if results.empty:
            st.warning("ğŸ” ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            cols = st.columns(min(3, len(results)))
            idx = 0
            for _, row in results.iterrows():
                with cols[idx % len(cols)]:
                    product_card(row)
                idx += 1
    elif not queries:
        st.info("ì œí’ˆì½”ë“œ ë˜ëŠ” ì œí’ˆëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# =====================================
# ì‚¬ì´ë“œë°” ë‚´ë¹„
# =====================================
with st.sidebar:
    st.markdown(f"### {APP_TITLE}")
    st.caption(PROJECT_SUBTITLE)
    sel = st.radio(
        "ì„¹ì…˜",
        ["ğŸ’¬ ì±—ë´‡", "ğŸ“˜ ì œí’ˆë°±ì„œ"],
        index=0 if st.session_state.route == "CHAT" else 1,
        label_visibility="collapsed"
    )
    st.markdown("---")
    st.caption("Â© Internal Use Only")

if sel.startswith("ğŸ’¬"):
    st.session_state.route = "CHAT"
    page_chatbot()
else:
    st.session_state.route = "WHITEPAPER"
    page_whitepaper()
